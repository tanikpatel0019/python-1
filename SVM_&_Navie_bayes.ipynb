{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Information Gain, and how is it used in Decision Trees?\n",
        "- Information Gain measures the reduction in entropy within a dataset after splitting it based on a specific feature. It is used in Decision Tree algorithms to select the best attribute for splitting at each node, choosing the feature that yields the highest IG.\n",
        "\n",
        "2. What is the difference between Gini Impurity and Entropy?\n",
        "- Gini Impurity and Entropy both measure node impurity in decision trees, but differ in calculation and range: Gini Impurity (0 to 0.5) measures misclassification probability, favoring simpler splits and being computationally faster; Entropy (0 to 1) measures disorder/randomness using logarithms, providing a more nuanced view of uncertainty, though slightly slower.\n",
        "\n",
        "3. What is Pre-Pruning in Decision Trees?\n",
        "- Pre-pruning in decision trees, also known as early stopping, is a technique that halts the tree's growth during its construction to prevent it from becoming too complex and overfitting the training data."
      ],
      "metadata": {
        "id": "JrAu3CTfUmSs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#4. Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances (practical).\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "def train_decision_tree_and_print_importances():\n",
        "    \"\"\"\n",
        "    Trains a Decision Tree Classifier with Gini impurity and prints feature importances.\n",
        "    \"\"\"\n",
        "    iris = load_iris()\n",
        "    X, y = iris.data, iris.target\n",
        "    feature_names = iris.feature_names\n",
        "    dt_classifier = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "\n",
        "    print(\"Training the Decision Tree Classifier...\")\n",
        "    dt_classifier.fit(X, y)\n",
        "    print(\"Training complete.\")\n",
        "\n",
        "    importances = dt_classifier.feature_importances_\n",
        "\n",
        "    print(\"\\n--- Feature Importances ---\")\n",
        "    feature_importance_df = pd.DataFrame({\n",
        "        'Feature': feature_names,\n",
        "        'Importance': importances\n",
        "    })\n",
        "    feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "    print(feature_importance_df)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_decision_tree_and_print_importances()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75wT5FpMWYgw",
        "outputId": "59ac5ed8-b83c-43ee-9c0d-dde3e6c4a5f4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training the Decision Tree Classifier...\n",
            "Training complete.\n",
            "\n",
            "--- Feature Importances ---\n",
            "             Feature  Importance\n",
            "2  petal length (cm)    0.564056\n",
            "3   petal width (cm)    0.422611\n",
            "0  sepal length (cm)    0.013333\n",
            "1   sepal width (cm)    0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What is a Support Vector Machine (SVM)?\n",
        "- A Support Vector Machine (SVM) is a powerful supervised machine learning algorithm used for classification and regression.\n",
        "\n",
        "6. What is the Kernel Trick in SVM?\n",
        "- The Kernel Trick in SVM is a powerful technique that allows Support Vector Machines to efficiently classify non-linearly separable data by implicitly mapping it into a higher-dimensional space, where it becomes linearly separable, without actually computing the coordinates in that new space, saving massive computational effort.\n"
      ],
      "metadata": {
        "id": "u0LSM5OJXkNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#7. Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies.\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # 70% training, 30% testing\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "svm_linear = SVC(kernel='linear', random_state=42)\n",
        "svm_linear.fit(X_train_scaled, y_train)\n",
        "\n",
        "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
        "svm_rbf.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred_linear = svm_linear.predict(X_test_scaled)\n",
        "y_pred_rbf = svm_rbf.predict(X_test_scaled)\n",
        "\n",
        "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
        "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "print(f\"Accuracy of the Linear SVM classifier: {accuracy_linear:.4f}\")\n",
        "print(f\"Accuracy of the RBF SVM classifier: {accuracy_rbf:.4f}\")\n",
        "\n",
        "if accuracy_linear > accuracy_rbf:\n",
        "    print(\"\\nThe Linear SVM classifier performed better on this dataset.\")\n",
        "elif accuracy_rbf > accuracy_linear:\n",
        "    print(\"\\nThe RBF SVM classifier performed better on this dataset.\")\n",
        "else:\n",
        "    print(\"\\nBoth SVM classifiers achieved the same accuracy.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVmvj7q5XX5D",
        "outputId": "2d28f5c0-6785-4c63-a665-ac388ffa7cbd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the Linear SVM classifier: 0.9815\n",
            "Accuracy of the RBF SVM classifier: 0.9815\n",
            "\n",
            "Both SVM classifiers achieved the same accuracy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
        "- The Naive Bayes classifier is a fast, simple probabilistic model that predicts class membership by applying Bayes' theorem. It's called \"naive\" because this assumption of feature independence.\n",
        "\n",
        "9. Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve\n",
        "Bayes, and Bernoulli Naïve Bayes.\n",
        "- Gaussian, Multinomial, and Bernoulli Naive Bayes differ primarily in the type of data they handle: Gaussian for continuous data, Multinomial for discrete counts, and Bernoulli for binary features, all applying Bayes' theorem but with different underlying probability models suited to their data type.\n",
        "\n",
        "10. Breast Cancer Dataset\n",
        "Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer\n",
        "dataset and evaluate accuracy.\n",
        "Hint:Use GaussianNB() from sklearn.naive_bayes and the Breast Cancer dataset from\n",
        "sklearn.datasets.\n",
        "(Include your Python code and output in the code box below.)\n",
        "-\n"
      ],
      "metadata": {
        "id": "xtts5iFmZgOD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#10. Breast Cancer Dataset\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "gnb = GaussianNB()\n",
        "\n",
        "gnb.fit(X_train, y_train)\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of Gaussian Naïve Bayes: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03gG3zQGYuEk",
        "outputId": "ebe1a17c-fbd0-46ce-c145-388c36a2ac07"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Gaussian Naïve Bayes: 0.9737\n"
          ]
        }
      ]
    }
  ]
}