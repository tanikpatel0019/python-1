{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is K-Nearest Neighbors (KNN) and how does it work in both classification and regression problems?\n",
        "- K-Nearest Neighbors (KNN) is a simple, non-parametric, lazy-learning supervised algorithm that predicts the label or value of a new data point based on the similarity of its nearest neighbors. It calculates distance to find the closest points, using majority voting for classification and averaging for regression.\n",
        "\n",
        "2. What is the Curse of Dimensionality and how does it affect KNN\n",
        "performance?\n",
        "- The Curse of Dimensionality refers to the exponential increase in volume associated with adding extra dimensions (features) to data, making it sparse and distance metrics less meaningful.\n",
        "\n",
        "3.  What is Principal Component Analysis (PCA)? How is it different from\n",
        "feature selection?\n",
        "- Principal Component Analysis is an unsupervised dimensionality reduction technique that transforms a large set of variables into a smaller, uncorrelated set called \"principal components\" while retaining maximum data variance.\n",
        "\n",
        "4. What are eigenvalues and eigenvectors in PCA, and why are they\n",
        "important?\n",
        "- In PCA, eigenvectors represent the directions of maximum variance in data, while eigenvalues are scalars indicating the magnitude of variance captured along those directionsIn PCA, eigenvectors represent the directions of maximum variance in data, while eigenvalues are scalars indicating the magnitude of variance captured along those directions.\n",
        "\n",
        "5. How do KNN and PCA complement each other when applied in a single\n",
        "pipeline?\n",
        "- When applied in a single pipeline, PCA and KNN complement each other by addressing each other's weaknesses: PCA provides dimensionality reduction and noise reduction, which significantly improves the computational efficiency and distance-based accuracy of the KNN algorithm."
      ],
      "metadata": {
        "id": "TrcqHweiihRx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#6. Train a KNN Classifier on the Wine dataset with and without feature scaling. Compare model accuracy in both cases.\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "knn_no_scaling = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "knn_no_scaling.fit(X_train, y_train)\n",
        "\n",
        "y_pred_no_scaling = knn_no_scaling.predict(X_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = knn_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "print(\"--- KNN Classifier Accuracy Comparison ---\")\n",
        "print(f\"Accuracy without feature scaling: {accuracy_no_scaling:.4f}\")\n",
        "print(f\"Accuracy with feature scaling:    {accuracy_scaled:.4f}\")\n",
        "\n",
        "if accuracy_scaled > accuracy_no_scaling:\n",
        "    print(\"\\nConclusion: Feature scaling significantly improved model accuracy.\")\n",
        "elif accuracy_no_scaling > accuracy_scaled:\n",
        "    print(\"\\nConclusion: Accuracy without scaling was higher (uncommon for KNN).\")\n",
        "else:\n",
        "    print(\"\\nConclusion: Feature scaling had no impact on the model accuracy in this case.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9G-PN618nn-",
        "outputId": "9b79ff83-39cc-49c9-f9f0-001eecdb07fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- KNN Classifier Accuracy Comparison ---\n",
            "Accuracy without feature scaling: 0.7407\n",
            "Accuracy with feature scaling:    0.9630\n",
            "\n",
            "Conclusion: Feature scaling significantly improved model accuracy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Train a PCA model on the Wine dataset and print the explained variance\n",
        "ratio of each principal component.\n",
        "- Training a PCA model on the wine dataset involves standardizing the 13 features, applying PCA, and analyzing the explained_variance_ratio_. PCA decomposes data, revealing that the first few principal components explain the majority of variance, often achieving over 80% with only two or three components."
      ],
      "metadata": {
        "id": "fNoGYoRO9ryY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "pca = PCA(n_components=None)\n",
        "pca.fit(X_scaled)\n",
        "print(\"Explained variance ratio of each principal component:\")\n",
        "for i, ratio in enumerate(pca.explained_variance_ratio_):\n",
        "    print(f\"PC{i + 1}: {ratio:.4f}\")\n",
        "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
        "print(\"\\nCumulative explained variance:\")\n",
        "for i, cum_ratio in enumerate(cumulative_variance):\n",
        "    print(f\"PC{i + 1}: {cum_ratio:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxW7tK26-EwU",
        "outputId": "590dc6ab-927c-4c0f-daf4-eba317015026"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained variance ratio of each principal component:\n",
            "PC1: 0.3620\n",
            "PC2: 0.1921\n",
            "PC3: 0.1112\n",
            "PC4: 0.0707\n",
            "PC5: 0.0656\n",
            "PC6: 0.0494\n",
            "PC7: 0.0424\n",
            "PC8: 0.0268\n",
            "PC9: 0.0222\n",
            "PC10: 0.0193\n",
            "PC11: 0.0174\n",
            "PC12: 0.0130\n",
            "PC13: 0.0080\n",
            "\n",
            "Cumulative explained variance:\n",
            "PC1: 0.3620\n",
            "PC2: 0.5541\n",
            "PC3: 0.6653\n",
            "PC4: 0.7360\n",
            "PC5: 0.8016\n",
            "PC6: 0.8510\n",
            "PC7: 0.8934\n",
            "PC8: 0.9202\n",
            "PC9: 0.9424\n",
            "PC10: 0.9617\n",
            "PC11: 0.9791\n",
            "PC12: 0.9920\n",
            "PC13: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#8. Train a KNN Classifier on the PCA-transformed dataset (retain top 2 components). Compare the accuracy with the original dataset.\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "pca = PCA(n_components=None)\n",
        "pca.fit(X_scaled)\n",
        "print(\"Explained variance ratio of each principal component:\")\n",
        "for i, ratio in enumerate(pca.explained_variance_ratio_):\n",
        "    print(f\"PC{i + 1}: {ratio:.4f}\")\n",
        "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
        "print(\"\\nCumulative explained variance:\")\n",
        "for i, cum_ratio in enumerate(cumulative_variance):\n",
        "    print(f\"PC{i + 1}: {cum_ratio:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chUnejdj-nxe",
        "outputId": "29281636-42ab-4072-b33f-eed32d7e6429"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained variance ratio of each principal component:\n",
            "PC1: 0.3620\n",
            "PC2: 0.1921\n",
            "PC3: 0.1112\n",
            "PC4: 0.0707\n",
            "PC5: 0.0656\n",
            "PC6: 0.0494\n",
            "PC7: 0.0424\n",
            "PC8: 0.0268\n",
            "PC9: 0.0222\n",
            "PC10: 0.0193\n",
            "PC11: 0.0174\n",
            "PC12: 0.0130\n",
            "PC13: 0.0080\n",
            "\n",
            "Cumulative explained variance:\n",
            "PC1: 0.3620\n",
            "PC2: 0.5541\n",
            "PC3: 0.6653\n",
            "PC4: 0.7360\n",
            "PC5: 0.8016\n",
            "PC6: 0.8510\n",
            "PC7: 0.8934\n",
            "PC8: 0.9202\n",
            "PC9: 0.9424\n",
            "PC10: 0.9617\n",
            "PC11: 0.9791\n",
            "PC12: 0.9920\n",
            "PC13: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#9. Train a KNN Classifier with different distance metrics (euclidean, manhattan) on the scaled Wine dataset and compare the results.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
        "knn_euclidean.fit(X_train_scaled, y_train)\n",
        "y_pred_euclidean = knn_euclidean.predict(X_test_scaled)\n",
        "accuracy_euclidean = accuracy_score(y_test, y_pred_euclidean)\n",
        "\n",
        "print(\"--- Results with Euclidean Distance (L2) ---\")\n",
        "print(f\"Accuracy: {accuracy_euclidean:.4f}\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_euclidean))\n",
        "print(\"-\" * 40)\n",
        "\n",
        "knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n",
        "knn_manhattan.fit(X_train_scaled, y_train)\n",
        "y_pred_manhattan = knn_manhattan.predict(X_test_scaled)\n",
        "accuracy_manhattan = accuracy_score(y_test, y_pred_manhattan)\n",
        "\n",
        "print(\"--- Results with Manhattan Distance (L1) ---\")\n",
        "print(f\"Accuracy: {accuracy_manhattan:.4f}\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_manhattan))\n",
        "print(\"-\" * 40)\n",
        "\n",
        "print(\"--- Comparison Summary ---\")\n",
        "print(f\"Euclidean Distance Accuracy: {accuracy_euclidean:.4f}\")\n",
        "print(f\"Manhattan Distance Accuracy: {accuracy_manhattan:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRPYc01x_dUm",
        "outputId": "a84ec5ac-1304-45ac-bc43-8456f60936b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Results with Euclidean Distance (L2) ---\n",
            "Accuracy: 0.9630\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      1.00      0.97        19\n",
            "           1       1.00      0.90      0.95        21\n",
            "           2       0.93      1.00      0.97        14\n",
            "\n",
            "    accuracy                           0.96        54\n",
            "   macro avg       0.96      0.97      0.96        54\n",
            "weighted avg       0.97      0.96      0.96        54\n",
            "\n",
            "----------------------------------------\n",
            "--- Results with Manhattan Distance (L1) ---\n",
            "Accuracy: 0.9630\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      1.00      0.97        19\n",
            "           1       1.00      0.90      0.95        21\n",
            "           2       0.93      1.00      0.97        14\n",
            "\n",
            "    accuracy                           0.96        54\n",
            "   macro avg       0.96      0.97      0.96        54\n",
            "weighted avg       0.97      0.96      0.96        54\n",
            "\n",
            "----------------------------------------\n",
            "--- Comparison Summary ---\n",
            "Euclidean Distance Accuracy: 0.9630\n",
            "Manhattan Distance Accuracy: 0.9630\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#10. You are working with a high-dimensional gene expression dataset to classify patients with different types of cancer. Due to the large number of features and a small number of samples, traditional models overfit.\n"
      ],
      "metadata": {
        "id": "urc4aqqBAm0b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}